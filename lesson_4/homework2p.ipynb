{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E02. Folding batch norm into Linear layer Weights & Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16424\n"
     ]
    }
   ],
   "source": [
    "# Let's train a deeper network\n",
    "# The classes we create here are the same API as nn.Module in PyTorch\n",
    "\n",
    "class Linear:\n",
    "  \n",
    "  def __init__(self, fan_in, fan_out, bias=True):\n",
    "    self.in_features = fan_in\n",
    "    self.out_features = fan_out\n",
    "    self.weight = torch.randn((self.in_features, self.out_features), generator=g) #/ fan_in**0.5\n",
    "    # self.weight = torch.zeros(fan_in, fan_out) \n",
    "    self.bias = torch.zeros(self.out_features) if bias else None\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    self.out = x @ self.weight\n",
    "    if self.bias is not None:\n",
    "      self.out += self.bias\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "\n",
    "class BatchNorm1d:\n",
    "  \n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.momentum = momentum\n",
    "    self.training = True\n",
    "    # parameters (trained with backprop)\n",
    "    # self.gamma = torch.zeros(dim)\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "    # buffers (trained with a running 'momentum update')\n",
    "    self.running_mean = torch.zeros(dim)\n",
    "    self.running_var = torch.ones(dim)\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    if self.training:\n",
    "      self.xmean = x.mean(0, keepdim=True) # batch mean\n",
    "      self.xvar = x.var(0, keepdim=True) # batch variance\n",
    "    else:\n",
    "      self.xmean = self.running_mean\n",
    "      self.xvar = self.running_var\n",
    "    xhat = (x - self.xmean) / torch.sqrt(self.xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    # update the buffers\n",
    "    if self.training:\n",
    "      with torch.no_grad():\n",
    "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * self.xmean\n",
    "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * self.xvar\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "class Tanh:\n",
    "  def __call__(self, x):\n",
    "    self.out = torch.tanh(x)\n",
    "    return self.out\n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 100 # the number of neurons in the hidden layer of the MLP\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "\n",
    "layers = [\n",
    "  Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(           n_hidden, vocab_size, bias=False), BatchNorm1d(vocab_size), \n",
    "]\n",
    "\n",
    "with torch.no_grad():\n",
    "  # last layer: make less confident\n",
    "  # layers[-1].weight *= 0.1\n",
    "  layers[-1].gamma *= 0.1\n",
    "  # all other layers: apply gain\n",
    "  for layer in layers[:-1]:\n",
    "    if isinstance(layer, Linear):\n",
    "      layer.weight *= 1.0 #5/3\n",
    "\n",
    "parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.3138\n"
     ]
    }
   ],
   "source": [
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "# Update-to-data ratio\n",
    "ud = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xb] # embed the characters into vectors\n",
    "    x = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "    for layer in layers:\n",
    "        x = layer(x)\n",
    "    loss = F.cross_entropy(x, Yb) # loss function\n",
    "\n",
    "    # backward pass\n",
    "    # for layer in layers:\n",
    "        # layer.out.retain_grad() # AFTER_DEBUG: would take out retain_graph\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    lr = 1.0\n",
    "    # lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    # track stats\n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())\n",
    "    with torch.no_grad():\n",
    "        ud.append([((lr*p.grad).std() / p.data.std()).log10().item() for p in parameters])\n",
    "    if i >= 1000:\n",
    "        break # AFTER_DEBUG: would take out obviously to run full optimization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in layers:\n",
    "    if isinstance(layer, BatchNorm1d):\n",
    "        layer.training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5])\n",
      "tensor([[ 8,  5, 14]])\n",
      "tensor([7])\n",
      "tensor([[[-1.4627,  0.9584, -0.8036, -1.4208,  1.1574, -0.7327,  0.9706,\n",
      "          -1.0113, -0.7093,  1.3238],\n",
      "         [ 0.1966,  0.4489,  1.1327, -0.9615,  0.8308, -0.0270,  1.2264,\n",
      "           0.5786, -1.7541, -1.0490],\n",
      "         [-0.0664, -0.3270, -1.9017, -0.5343,  0.1581,  0.7689,  0.8360,\n",
      "          -1.1002, -0.8201,  0.0174]]])\n",
      "tensor([[-1.4627,  0.9584, -0.8036, -1.4208,  1.1574, -0.7327,  0.9706, -1.0113,\n",
      "         -0.7093,  1.3238,  0.1966,  0.4489,  1.1327, -0.9615,  0.8308, -0.0270,\n",
      "          1.2264,  0.5786, -1.7541, -1.0490, -0.0664, -0.3270, -1.9017, -0.5343,\n",
      "          0.1581,  0.7689,  0.8360, -1.1002, -0.8201,  0.0174]])\n",
      "tensor([[-15.0269,   5.2077,  -2.7655,  -7.2338,   0.4678,  -7.5516, -10.3462,\n",
      "          -4.6439,  -0.2803,   0.0743,   6.7710,  13.8418,   3.3735,   7.7241,\n",
      "          -3.3188,  -2.4831,  -6.1476,  -3.8310,  -0.2850,  -8.0273,  -6.3942,\n",
      "          -0.1582,  -0.2489,   1.6168,  -6.0590,  -0.1325,  -0.4848,  -5.2684,\n",
      "           0.8530,  -2.8547,  10.1303,  -1.4199,  -8.2744,  -3.8879,   0.0909,\n",
      "           1.2204,  -0.4631,  -7.2766,  -3.1053,  -6.1127,   4.3007,   4.4278,\n",
      "         -11.9815,  -3.2528,   0.6221,  -0.8933,   3.7486,   4.2441,   6.2293,\n",
      "           4.7437,  -0.9173,  -1.1752,  -4.9766,  -3.6568,   1.0435,   1.0646,\n",
      "          -0.5254, -10.3634,  -0.9427,  -2.1216,   8.8580,  -8.7351,   1.9544,\n",
      "          -3.3709,   1.3823,   0.0566,  -0.6699, -15.7999,  -7.8289,  -2.5990,\n",
      "          -0.5710,   1.5115, -12.1682,   3.2511,   4.7756,   9.9599,  -1.6491,\n",
      "          -9.8479,   0.0667,  -8.0633,  -7.7878, -10.3274,  -7.5210,   9.2250,\n",
      "           2.4408,   6.5379,   1.4603,   1.5608,  -0.3554,  -3.9887,   2.6132,\n",
      "          -1.8356,  -5.1777,   8.8555,  -3.6763,   3.6632,  -4.2121,  -7.6750,\n",
      "           0.3719,   8.9533]]) <class '__main__.Linear'>\n",
      "tensor([[-1.2995,  0.3919,  0.0431, -1.7861,  0.2265, -0.9747, -1.6103, -0.3442,\n",
      "          0.9947,  0.0622,  1.7110,  1.4984,  0.1674,  1.7259, -0.4804, -0.7882,\n",
      "         -1.2727,  0.3393,  0.2982, -1.1001, -0.2605,  0.1389,  0.0096, -0.9884,\n",
      "         -1.1252,  0.3007, -0.3896, -1.1593,  1.3266, -0.7473,  0.4874, -0.4708,\n",
      "         -1.1736, -0.0895,  0.7904,  0.2268,  0.5327, -0.8785, -0.6315, -1.3982,\n",
      "          1.1357,  1.4644, -1.8951, -0.5052, -0.3492,  0.1328,  0.5543,  1.3246,\n",
      "          0.9483, -0.0931, -0.4570, -0.0559, -1.3180, -0.8360, -0.2913, -0.0808,\n",
      "         -0.2664, -1.0919, -0.9746, -0.3473,  2.8687, -2.3206,  0.1082, -0.8290,\n",
      "          0.1074,  0.4277, -0.1042, -2.5581, -1.0303,  0.0656, -0.2357,  0.1294,\n",
      "         -3.2755,  0.6327,  0.3239,  2.7539, -0.5583, -1.1867,  0.5230, -1.3267,\n",
      "         -1.0619, -1.4216, -1.5685,  0.9357,  0.5689,  1.4193,  0.5628, -0.1341,\n",
      "         -0.6241,  0.3811,  0.1452, -0.3391, -1.0549,  0.6018, -0.7625,  1.0140,\n",
      "         -0.2948, -1.0634,  0.7480,  1.2658]]) <class '__main__.BatchNorm1d'>\n",
      "False\n",
      "tensor([[-6.5576e+00,  1.3950e+00, -2.5661e+00,  1.3036e+00, -1.4343e+00,\n",
      "         -4.1476e+00, -3.0860e+00, -9.4288e-01, -6.5180e+00,  1.5034e+00,\n",
      "         -1.4972e+00, -5.4386e+00,  4.5534e+00,  4.5315e-02, -8.5394e-01,\n",
      "          3.6834e+00,  5.5219e-01, -5.3121e+00, -1.5235e+00, -1.3334e+00,\n",
      "         -1.1218e+00, -3.7030e+00, -2.7683e+00,  3.9219e+00,  1.0551e-01,\n",
      "         -1.3317e+00,  3.6722e+00, -4.8861e-01, -3.2280e+00, -5.4035e-01,\n",
      "          1.5945e+00,  6.4689e-01, -1.7495e+00, -2.8896e-01, -3.6210e+00,\n",
      "         -4.1649e+00, -2.6892e+00, -2.3166e+00, -1.9930e+00, -4.9235e-01,\n",
      "         -1.5010e+00, -1.3864e-01, -5.4482e-01, -8.2676e-01, -8.3898e-02,\n",
      "         -1.5220e+00, -3.0565e+00, -9.7973e-01,  1.8926e+00,  3.1827e+00,\n",
      "          2.7510e+00,  1.1336e-01, -6.4254e-03,  1.0849e+00,  3.6235e+00,\n",
      "          4.6787e-01,  5.6915e-01,  7.7208e-01,  1.5087e+00, -7.2623e-03,\n",
      "         -2.6353e+00,  5.1627e+00,  1.3485e+00,  7.1517e-01,  1.9590e+00,\n",
      "         -1.5267e+00, -7.7354e-01, -1.1851e+00, -3.2122e+00,  2.8181e+00,\n",
      "          1.0127e+00,  4.7928e-01,  1.0463e+00, -1.8314e+00,  2.8355e+00,\n",
      "         -3.6393e-01,  4.3821e+00, -1.9366e+00, -4.6429e-01, -1.8252e+00,\n",
      "         -6.0843e-01, -4.5632e-01,  1.4726e+00,  2.7215e+00,  3.4960e-01,\n",
      "         -7.6302e-01, -1.1664e+00,  4.0986e+00,  1.5933e+00, -3.2975e+00,\n",
      "          2.8180e+00, -2.2575e+00,  1.6957e+00,  2.6824e+00, -8.4982e-01,\n",
      "          3.0238e-01, -1.8201e+00, -1.9655e+00, -1.8107e+00,  2.1300e+00]])\n",
      "tensor([[ 37.2997,  68.0991,  50.7025,  62.1672,  27.8554,  33.3301,  30.0475,\n",
      "          42.0081,  81.8911,  37.9275,  38.1447, 123.6968,  50.2967,  21.6351,\n",
      "          37.3455,  45.2832,  29.2988,  20.0202,  25.2795,  27.1009,  24.8188,\n",
      "          94.1589,  59.5558,  51.1373,  44.4238,  62.1585,  47.5209,  32.8609,\n",
      "          27.1488,  32.0739,  38.3047,  14.9142,  22.5866,  35.0713,  36.8966,\n",
      "          82.5256,  43.5584,  42.0958,  28.8389,  29.3180,  37.8425,  22.6278,\n",
      "          22.8860,  32.8893,  23.3621,  37.1566,  60.1787,  15.1690,  14.3124,\n",
      "          33.3823,  26.5815,  21.7730,  30.0138,  29.2471,  56.5021,  30.5699,\n",
      "          24.4070,  78.7646,  55.2687,  26.2854,  32.0776,  55.6183,  25.0776,\n",
      "          14.3428,  61.2744,  23.4507,  27.4928,  26.2644,  31.1248,  36.6982,\n",
      "          16.5703,  45.7994,  24.2678,  41.7984,  39.3088,  20.8067,  57.6448,\n",
      "          29.4149,  37.1791,  31.6133,  41.4093,  38.7563,  34.4267,  19.7743,\n",
      "          12.7666,  23.7648,  36.4787,  37.8289,  38.7645,  26.1612,  26.2825,\n",
      "          40.5512,  31.4670,  54.7829,  12.3489,  17.8493,  24.4890,  67.7076,\n",
      "          17.0631,  33.1926]])\n",
      "tensor([[-0.8616,  0.3730,  0.0431, -0.9453,  0.2227, -0.7507, -0.9232, -0.3312,\n",
      "          0.7594,  0.0621,  0.9368,  0.9049,  0.1658,  0.9386, -0.4465, -0.6574,\n",
      "         -0.8545,  0.3268,  0.2896, -0.8005, -0.2548,  0.1380,  0.0096, -0.7567,\n",
      "         -0.8094,  0.2919, -0.3710, -0.8208,  0.8684, -0.6335,  0.4521, -0.4389,\n",
      "         -0.8254, -0.0893,  0.6586,  0.2229,  0.4875, -0.7057, -0.5591, -0.8850,\n",
      "          0.8130,  0.8985, -0.9558, -0.4662, -0.3356,  0.1320,  0.5038,  0.8679,\n",
      "          0.7390, -0.0929, -0.4276, -0.0558, -0.8663, -0.6837, -0.2833, -0.0807,\n",
      "         -0.2603, -0.7976, -0.7507, -0.3339,  0.9936, -0.9809,  0.1077, -0.6799,\n",
      "          0.1070,  0.4034, -0.1039, -0.9881, -0.7740,  0.0655, -0.2314,  0.1287,\n",
      "         -0.9971,  0.5599,  0.3130,  0.9919, -0.5067, -0.8296,  0.4800, -0.8684,\n",
      "         -0.7864, -0.8899, -0.9168,  0.7332,  0.5145,  0.8895,  0.5100, -0.1333,\n",
      "         -0.5540,  0.3637,  0.1442, -0.3267, -0.7837,  0.5383, -0.6426,  0.7674,\n",
      "         -0.2866, -0.7870,  0.6340,  0.8527]]) <class '__main__.Tanh'>\n",
      "tensor([[ -7.9621,  -6.2577,  12.5575,   2.0232,  -8.7317,   0.6412,   4.2149,\n",
      "          -0.0813,  -3.1891,  -6.4687,   9.0115,   4.5531,  -7.6089,  -4.3588,\n",
      "          -7.7297,  -5.6544,  -5.1649,  -0.7567,   0.9609,   0.3258,  13.4861,\n",
      "          10.2950,   6.7942,  -2.7593,   2.8015,  -5.0688,  -7.5338,  -5.2286,\n",
      "         -18.6757,   4.4210,   2.4556,   6.6568,  -4.2633,   2.4306,   1.9890,\n",
      "          -0.5313, -13.7893,  -7.3972,   4.7563,  -0.8259,  15.3853,   4.9136,\n",
      "          -1.2613,   1.5656,   9.1098,   6.3633,   6.6738,  -6.7184,  -1.6032,\n",
      "          10.1020,   0.8917,  -3.8639,  -6.0376,   8.2279,   5.6578, -13.7741,\n",
      "           6.5911,   6.0597,  -0.3508,   0.2240,   5.2736,   6.1323,  10.2762,\n",
      "           2.1289,  -1.7206,   2.7814,  -4.6986,  -0.7600,  -3.3204,   0.3419,\n",
      "          -4.8793, -15.1293,  -7.8854,   3.3274,  -6.9562,  12.6348,  -8.2780,\n",
      "          -1.3074,  -5.9552,   9.0009,   0.9646,   8.4272,  -3.7003,   1.5184,\n",
      "          -6.3261,   3.6989,  -5.9031,   8.4045,   8.3610,   0.5153,  -4.2231,\n",
      "           2.3170,   0.6556,   2.2950,  -0.5645,   9.3527,   0.3609,   3.5128,\n",
      "          -0.5507,   0.5515]]) <class '__main__.Linear'>\n",
      "tensor([[-0.6216, -0.8623,  1.3372,  0.4152, -2.0473, -0.2547,  0.5486,  0.4296,\n",
      "          0.1738, -1.2706,  0.9138,  1.1970, -0.7395, -1.0361, -1.5049,  0.0885,\n",
      "         -0.3186, -0.2642,  0.1223, -0.4912,  1.6636,  1.1809,  1.4957, -0.3898,\n",
      "          0.4454, -1.5087, -0.6519, -0.3353, -2.7543,  0.5895,  0.2950,  0.7666,\n",
      "         -0.8122,  0.3126,  0.2108, -0.1140, -2.3085, -0.9084,  0.8197,  0.3150,\n",
      "          1.8226,  0.0419, -0.8259,  0.5514,  0.2626,  0.7765,  1.5283, -1.1189,\n",
      "         -0.7459,  1.0396,  0.2847, -1.1284, -0.6044,  0.9298,  0.7398, -1.6894,\n",
      "          0.4861,  1.3083,  0.0070,  0.3149, -0.0397,  1.0324,  0.7738,  0.0770,\n",
      "          0.0670,  0.5296, -0.4829, -0.7314, -0.7344,  0.5394, -0.5797, -1.8203,\n",
      "         -3.3461,  0.6561, -0.9240,  0.5899, -0.6655, -0.2414, -0.6539,  1.8181,\n",
      "         -0.4576,  1.1138, -0.4862, -0.1008, -1.4501, -0.5788, -1.6073,  1.0773,\n",
      "          1.3866,  0.0681, -1.2017,  0.0510, -0.1929,  0.3964, -0.6557,  1.4939,\n",
      "         -0.3525,  0.4716, -0.4546, -0.4103]]) <class '__main__.BatchNorm1d'>\n",
      "False\n",
      "tensor([[-3.6498, -3.3082,  2.9791,  0.4074, -0.5053,  1.1397,  0.9322, -2.9566,\n",
      "         -2.7082, -2.5671,  0.6763, -1.2077, -0.5565, -0.4383, -1.5031, -1.5673,\n",
      "         -0.3392, -0.6187,  1.6146,  3.2593,  0.2549,  0.6320, -0.2770,  2.6037,\n",
      "          2.4044,  1.8686, -0.9546,  0.2693, -3.5319,  1.0237,  1.0724,  0.3357,\n",
      "         -1.6600,  1.5438,  1.3942, -0.1389, -1.1074, -3.1601,  0.4755, -1.1581,\n",
      "          0.6345,  2.4164,  1.6864, -1.0727,  0.0544,  0.3917,  1.3544, -0.1313,\n",
      "          3.7130,  0.3434,  1.6136,  1.3217,  3.7081,  2.9872,  2.5570,  0.6793,\n",
      "          0.4405, -0.6974,  0.3066,  0.6874,  3.7686, -0.6490,  2.7095,  0.2659,\n",
      "         -1.9288,  0.1578, -1.6514,  2.3958, -0.7269, -3.9648, -0.1085, -0.5788,\n",
      "          0.2738, -1.0234,  0.7825,  2.2992, -1.0029,  0.4310, -1.0338,  0.7391,\n",
      "          2.9683,  1.0877,  3.6397,  1.8360,  1.4447,  2.1252, -0.0719, -1.1738,\n",
      "         -0.6271, -1.6007,  1.7711,  1.9912,  2.0744,  0.0460,  2.6849,  1.0458,\n",
      "          3.3980,  0.7683,  0.0220,  4.1739]])\n",
      "tensor([[ 31.7569,  51.5780,  38.5921,  37.0543,  28.9699,  17.6669,  24.9783,\n",
      "          28.0477,  45.1394,  27.0496,  81.4358,  51.0439,  47.6046,  23.6834,\n",
      "          22.3342,  22.2811,  45.3672,  81.4357,  55.0705,  28.8802, 105.5996,\n",
      "          48.4512,  37.8604,  55.5294,  17.7009,  49.7964,  31.0375, 116.4401,\n",
      "          46.8965,  21.8050,  40.3973,  62.5231,  31.0272,  66.0184,  72.1099,\n",
      "          26.6366,  33.4930,  61.2108,  29.4409,  22.2360,  34.4798,  41.2754,\n",
      "          27.8691,  24.9961,  41.8836,  43.5634,  33.1730,  17.9656,  28.2998,\n",
      "          37.9260,  35.1743,  43.1067,  36.5952,  30.6169,  22.1324,  76.1341,\n",
      "          47.8184,  24.5938,  26.6242,  45.1585,  63.1088,  64.4992,  56.7526,\n",
      "          18.5643,  44.5203,  38.3300,  22.6648,  59.9769,  30.4563,  32.5543,\n",
      "          37.8380,  31.9854,  15.7842,  53.9742,  39.3854,  36.4607,  23.8347,\n",
      "          22.9948,  40.0644,  33.1701,  20.8686,  49.1650,  42.5965,  27.0351,\n",
      "          44.9694,  43.9776,  22.8208,  74.6567,  51.4562,  30.7298,  35.2942,\n",
      "          21.0743,  24.2153,  34.0188,  18.5800,  35.4361,  27.5677,  20.7500,\n",
      "          19.9500,  50.0584]])\n",
      "tensor([[-0.5522, -0.6974,  0.8710,  0.3929, -0.9672, -0.2494,  0.4995,  0.4050,\n",
      "          0.1721, -0.8540,  0.7230,  0.8327, -0.6288, -0.7764, -0.9060,  0.0883,\n",
      "         -0.3083, -0.2583,  0.1217, -0.4551,  0.9307,  0.8278,  0.9044, -0.3712,\n",
      "          0.4181, -0.9067, -0.5729, -0.3232, -0.9919,  0.5295,  0.2867,  0.6449,\n",
      "         -0.6708,  0.3028,  0.2077, -0.1135, -0.9804, -0.7203,  0.6749,  0.3050,\n",
      "          0.9491,  0.0419, -0.6783,  0.5016,  0.2567,  0.6507,  0.9101, -0.8072,\n",
      "         -0.6327,  0.7777,  0.2772, -0.8105, -0.5402,  0.7305,  0.6291, -0.9341,\n",
      "          0.4511,  0.8638,  0.0070,  0.3049, -0.0396,  0.7749,  0.6491,  0.0769,\n",
      "          0.0669,  0.4851, -0.4485, -0.6239, -0.6258,  0.4925, -0.5225, -0.9489,\n",
      "         -0.9975,  0.5758, -0.7278,  0.5298, -0.5820, -0.2368, -0.5743,  0.9486,\n",
      "         -0.4281,  0.8054, -0.4512, -0.1005, -0.8957, -0.5218, -0.9228,  0.7922,\n",
      "          0.8824,  0.0680, -0.8342,  0.0509, -0.1905,  0.3769, -0.5755,  0.9040,\n",
      "         -0.3386,  0.4395, -0.4257, -0.3887]]) <class '__main__.Tanh'>\n",
      "tensor([[ 17.2856,  -5.6099,   1.1031,  -4.4837,  -3.2547,  -1.4631,   8.6410,\n",
      "           1.9587,  -7.1034,  -9.6503,  -9.1587,  12.8629,  -2.7425, -13.4565,\n",
      "           5.0563,  -2.5002,   6.8289,  -4.4919,  -1.8859,  -9.4795,   5.1125,\n",
      "          -5.9395,   6.8836,   5.7395,   2.6630,  -3.5139, -11.2749]]) <class '__main__.Linear'>\n",
      "tensor([[ 3.8958,  2.5423, -0.4113, -0.3557,  0.5119,  2.0522, -2.0683, -1.1167,\n",
      "         -1.3267,  2.6212, -2.2079, -0.9337,  1.3606, -0.3842,  1.6836,  1.0674,\n",
      "         -2.0706, -2.2930,  1.0738,  0.0307,  0.6055, -0.6419, -1.0214, -1.8177,\n",
      "         -1.3475,  1.4597, -0.8300]]) <class '__main__.BatchNorm1d'>\n",
      "False\n",
      "tensor([[ 1.5346e+00,  6.7551e-02,  1.6205e+00, -3.4870e-02, -1.8075e+00,\n",
      "          3.1732e+00,  3.4599e+00, -1.4510e+00, -1.3618e+00,  5.2106e-02,\n",
      "          8.0478e-04,  9.4441e-01, -1.6807e+00, -1.5026e-01,  1.5035e+00,\n",
      "         -1.2950e-01,  1.4773e-01, -3.9145e-01,  3.0524e-01,  6.3931e-01,\n",
      "          1.9375e+00, -2.8645e+00,  1.2677e+00, -1.0787e+00, -7.6193e-02,\n",
      "          1.3439e+00, -2.7690e+00]])\n",
      "tensor([[158.0826, 102.4032,  62.8609, 111.4520,  45.5519,  92.0556,  33.3601,\n",
      "          29.7569,  32.0589,  84.3766,  80.6482,  84.4762,  29.3570,  72.5985,\n",
      "          50.4526,  88.7380,  38.2636,  31.9483,  54.4590,  67.5056,  28.5524,\n",
      "          35.3965,  62.3699,  58.3742,  44.0271,  47.4382,  66.2029]])\n",
      "tensor([[ 3.8958,  2.5423, -0.4113, -0.3557,  0.5119,  2.0522, -2.0683, -1.1167,\n",
      "         -1.3267,  2.6212, -2.2079, -0.9337,  1.3606, -0.3842,  1.6836,  1.0674,\n",
      "         -2.0706, -2.2930,  1.0738,  0.0307,  0.6055, -0.6419, -1.0214, -1.8177,\n",
      "         -1.3475,  1.4597, -0.8300]])\n",
      "tensor(5.8408)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # minibatch construct\n",
    "    ix = torch.tensor([5])\n",
    "    print(ix)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "    print(Xb)\n",
    "    print(Yb)\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xb] # embed the characters into vectors\n",
    "    print(emb)\n",
    "    x = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "    print(x)\n",
    "    for layer in layers:\n",
    "        x = layer(x)\n",
    "        print(x, type(layer))\n",
    "        if isinstance(layer, BatchNorm1d):\n",
    "            print(layer.training)\n",
    "            print(layer.xmean)\n",
    "            print(layer.xvar)\n",
    "    print(x)\n",
    "    loss = F.cross_entropy(x, Yb) # loss function\n",
    "    print(loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's remove the BatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_layers = [\n",
    "#   Linear(n_embd * block_size, n_hidden, bias=False), Tanh(),\n",
    "#   Linear(           n_hidden, n_hidden, bias=False), Tanh(),\n",
    "#   Linear(           n_hidden, vocab_size, bias=False), \n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8844/2377475961.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  gamma = torch.tensor(layers[i+1].gamma)\n",
      "/tmp/ipykernel_8844/2377475961.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  beta = torch.tensor(layers[i+1].beta)\n",
      "/tmp/ipykernel_8844/2377475961.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(layers[i+1].xmean)\n",
      "/tmp/ipykernel_8844/2377475961.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  weight = torch.tensor(layers[i].weight)\n",
      "/tmp/ipykernel_8844/2377475961.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.sqrt(torch.tensor(layers[i+1].xvar) + torch.tensor(layers[i+1].eps))\n"
     ]
    }
   ],
   "source": [
    "new_layers = []\n",
    "with torch.no_grad():\n",
    "    for i in range(len(layers)):\n",
    "        if isinstance(layers[i], Linear):\n",
    "            # Create a new Linear layer with the same parameters as the original layer\n",
    "            new_layer = Linear(layers[i].in_features, layers[i].out_features, bias = True)\n",
    "            \n",
    "            # Modify the weights and biases of the new layer\n",
    "            gamma = torch.tensor(layers[i+1].gamma)\n",
    "            beta = torch.tensor(layers[i+1].beta)\n",
    "            mean = torch.tensor(layers[i+1].xmean)\n",
    "            weight = torch.tensor(layers[i].weight)\n",
    "            std = torch.sqrt(torch.tensor(layers[i+1].xvar) + torch.tensor(layers[i+1].eps))\n",
    "            new_layer.weight = weight\n",
    "            new_layer.weight = (new_layer.weight / std) * gamma\n",
    "            new_layer.bias = beta - (gamma * (mean / std))\n",
    "            \n",
    "            new_layers.append(new_layer)\n",
    "        \n",
    "        if isinstance(layers[i], Tanh):\n",
    "            new_layer = Tanh()\n",
    "            new_layers.append(new_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5])\n",
      "tensor([[ 8,  5, 14]])\n",
      "tensor([7])\n",
      "tensor([[[-1.4627,  0.9584, -0.8036, -1.4208,  1.1574, -0.7327,  0.9706,\n",
      "          -1.0113, -0.7093,  1.3238],\n",
      "         [ 0.1966,  0.4489,  1.1327, -0.9615,  0.8308, -0.0270,  1.2264,\n",
      "           0.5786, -1.7541, -1.0490],\n",
      "         [-0.0664, -0.3270, -1.9017, -0.5343,  0.1581,  0.7689,  0.8360,\n",
      "          -1.1002, -0.8201,  0.0174]]])\n",
      "tensor([[-1.4627,  0.9584, -0.8036, -1.4208,  1.1574, -0.7327,  0.9706, -1.0113,\n",
      "         -0.7093,  1.3238,  0.1966,  0.4489,  1.1327, -0.9615,  0.8308, -0.0270,\n",
      "          1.2264,  0.5786, -1.7541, -1.0490, -0.0664, -0.3270, -1.9017, -0.5343,\n",
      "          0.1581,  0.7689,  0.8360, -1.1002, -0.8201,  0.0174]])\n",
      "tensor([[-1.2995,  0.3919,  0.0431, -1.7861,  0.2265, -0.9747, -1.6103, -0.3442,\n",
      "          0.9947,  0.0622,  1.7110,  1.4984,  0.1674,  1.7259, -0.4804, -0.7882,\n",
      "         -1.2727,  0.3393,  0.2982, -1.1001, -0.2605,  0.1389,  0.0096, -0.9884,\n",
      "         -1.1252,  0.3007, -0.3896, -1.1593,  1.3266, -0.7473,  0.4874, -0.4708,\n",
      "         -1.1736, -0.0895,  0.7904,  0.2268,  0.5327, -0.8785, -0.6315, -1.3982,\n",
      "          1.1357,  1.4644, -1.8951, -0.5052, -0.3492,  0.1328,  0.5543,  1.3246,\n",
      "          0.9483, -0.0931, -0.4570, -0.0559, -1.3180, -0.8360, -0.2913, -0.0808,\n",
      "         -0.2664, -1.0919, -0.9746, -0.3473,  2.8687, -2.3206,  0.1082, -0.8290,\n",
      "          0.1074,  0.4277, -0.1042, -2.5581, -1.0303,  0.0656, -0.2357,  0.1294,\n",
      "         -3.2755,  0.6327,  0.3239,  2.7539, -0.5583, -1.1867,  0.5230, -1.3267,\n",
      "         -1.0619, -1.4216, -1.5685,  0.9357,  0.5689,  1.4193,  0.5628, -0.1341,\n",
      "         -0.6241,  0.3811,  0.1452, -0.3391, -1.0549,  0.6018, -0.7625,  1.0140,\n",
      "         -0.2948, -1.0634,  0.7480,  1.2658]]) <class '__main__.Linear'>\n",
      "tensor([[-0.8616,  0.3730,  0.0431, -0.9453,  0.2227, -0.7507, -0.9232, -0.3312,\n",
      "          0.7594,  0.0621,  0.9368,  0.9049,  0.1658,  0.9386, -0.4465, -0.6574,\n",
      "         -0.8545,  0.3268,  0.2896, -0.8005, -0.2548,  0.1380,  0.0096, -0.7567,\n",
      "         -0.8094,  0.2919, -0.3710, -0.8208,  0.8684, -0.6335,  0.4521, -0.4389,\n",
      "         -0.8254, -0.0893,  0.6586,  0.2229,  0.4875, -0.7057, -0.5591, -0.8850,\n",
      "          0.8130,  0.8985, -0.9558, -0.4662, -0.3356,  0.1320,  0.5038,  0.8679,\n",
      "          0.7390, -0.0929, -0.4276, -0.0558, -0.8663, -0.6837, -0.2833, -0.0807,\n",
      "         -0.2603, -0.7976, -0.7507, -0.3339,  0.9936, -0.9809,  0.1077, -0.6799,\n",
      "          0.1070,  0.4034, -0.1039, -0.9881, -0.7740,  0.0655, -0.2314,  0.1287,\n",
      "         -0.9971,  0.5599,  0.3130,  0.9919, -0.5067, -0.8296,  0.4800, -0.8684,\n",
      "         -0.7864, -0.8899, -0.9168,  0.7332,  0.5145,  0.8895,  0.5100, -0.1333,\n",
      "         -0.5540,  0.3637,  0.1442, -0.3267, -0.7837,  0.5383, -0.6426,  0.7674,\n",
      "         -0.2866, -0.7870,  0.6340,  0.8527]]) <class '__main__.Tanh'>\n",
      "tensor([[-0.6216, -0.8623,  1.3372,  0.4152, -2.0473, -0.2547,  0.5486,  0.4296,\n",
      "          0.1738, -1.2706,  0.9138,  1.1970, -0.7395, -1.0361, -1.5049,  0.0885,\n",
      "         -0.3186, -0.2642,  0.1223, -0.4912,  1.6636,  1.1809,  1.4957, -0.3898,\n",
      "          0.4454, -1.5087, -0.6519, -0.3353, -2.7543,  0.5895,  0.2950,  0.7666,\n",
      "         -0.8122,  0.3126,  0.2108, -0.1140, -2.3085, -0.9084,  0.8197,  0.3150,\n",
      "          1.8226,  0.0419, -0.8259,  0.5514,  0.2626,  0.7765,  1.5283, -1.1189,\n",
      "         -0.7459,  1.0396,  0.2847, -1.1284, -0.6044,  0.9298,  0.7398, -1.6894,\n",
      "          0.4861,  1.3083,  0.0070,  0.3149, -0.0397,  1.0324,  0.7738,  0.0770,\n",
      "          0.0670,  0.5296, -0.4829, -0.7314, -0.7344,  0.5394, -0.5797, -1.8203,\n",
      "         -3.3461,  0.6561, -0.9240,  0.5899, -0.6655, -0.2414, -0.6539,  1.8181,\n",
      "         -0.4576,  1.1138, -0.4862, -0.1008, -1.4501, -0.5788, -1.6073,  1.0773,\n",
      "          1.3866,  0.0681, -1.2017,  0.0510, -0.1929,  0.3964, -0.6557,  1.4939,\n",
      "         -0.3525,  0.4716, -0.4546, -0.4103]]) <class '__main__.Linear'>\n",
      "tensor([[-0.5522, -0.6974,  0.8710,  0.3929, -0.9672, -0.2494,  0.4995,  0.4050,\n",
      "          0.1721, -0.8540,  0.7230,  0.8327, -0.6288, -0.7764, -0.9060,  0.0883,\n",
      "         -0.3083, -0.2583,  0.1217, -0.4551,  0.9307,  0.8278,  0.9044, -0.3712,\n",
      "          0.4181, -0.9067, -0.5729, -0.3232, -0.9919,  0.5295,  0.2867,  0.6449,\n",
      "         -0.6708,  0.3028,  0.2077, -0.1135, -0.9804, -0.7203,  0.6749,  0.3050,\n",
      "          0.9491,  0.0419, -0.6783,  0.5016,  0.2567,  0.6507,  0.9101, -0.8072,\n",
      "         -0.6327,  0.7777,  0.2772, -0.8105, -0.5402,  0.7305,  0.6291, -0.9341,\n",
      "          0.4511,  0.8638,  0.0070,  0.3049, -0.0396,  0.7749,  0.6491,  0.0769,\n",
      "          0.0669,  0.4851, -0.4485, -0.6239, -0.6258,  0.4925, -0.5225, -0.9489,\n",
      "         -0.9975,  0.5758, -0.7278,  0.5298, -0.5820, -0.2368, -0.5743,  0.9486,\n",
      "         -0.4281,  0.8054, -0.4512, -0.1005, -0.8957, -0.5218, -0.9228,  0.7922,\n",
      "          0.8824,  0.0680, -0.8342,  0.0509, -0.1905,  0.3769, -0.5755,  0.9040,\n",
      "         -0.3386,  0.4395, -0.4257, -0.3887]]) <class '__main__.Tanh'>\n",
      "tensor([[ 3.8958,  2.5423, -0.4113, -0.3557,  0.5119,  2.0522, -2.0683, -1.1167,\n",
      "         -1.3267,  2.6212, -2.2079, -0.9337,  1.3606, -0.3842,  1.6836,  1.0674,\n",
      "         -2.0706, -2.2930,  1.0738,  0.0307,  0.6055, -0.6419, -1.0214, -1.8177,\n",
      "         -1.3475,  1.4597, -0.8300]]) <class '__main__.Linear'>\n",
      "tensor(5.8408)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # minibatch construct\n",
    "    ix = torch.tensor([5])\n",
    "    print(ix)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "    print(Xb)\n",
    "    print(Yb)\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xb] # embed the characters into vectors\n",
    "    print(emb)\n",
    "    x = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "    print(x)\n",
    "    for layer in new_layers:\n",
    "        x = layer(x)\n",
    "        print(x, type(layer))\n",
    "    loss = F.cross_entropy(x, Yb) # loss function\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ttopt_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
